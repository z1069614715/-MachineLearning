# 决策树算法

**决策树ID3算法详解**`ID3算法选择要求是信息增益最大`
* 从根结点(root node)开始,对结点计算所有可能的特征的信息增益,选择信息增益最大的特征作为结点的特征.
* 由该特征的不同取值建立子节点,再对子结点递归地调用以上方法,构建决策树;直到所有特征的信息增益均很小或没有特征可以选择为止.
* 直到最后得到一个决策树

**决策树ID3具体代码实现**

    from math import log

    # 计算给定数据集的熵
    def calcShannonEnt(dataSet):
        dataLen = len(dataSet)
        labelCounts = {}
        for label in dataSet:
            labelCounts[label[-1]] = labelCounts.get(label[-1], 0) + 1 # 计算熵是计算结果类别的混乱度,所以只取最后一列
        res = 0.0
        for i in labelCounts:
            px = float(labelCounts[i]) / dataLen # 事件发生概率
            res -= px * log(px, 2)
        return res


    # 按照特征划分数据集
    def splitDataSet(dataSet, axis, value):
        retDataSet = []
        for line in dataSet:
            if line[axis] == value:
                reducedFeatVec = line[:axis]
                reducedFeatVec.extend(line[axis + 1:])
                retDataSet.append(reducedFeatVec)
        return retDataSet


    # 选择最好的数据集划分方式
    def chooseBestFeatureToSplit(dataSet):
        lenOfFeatures = len(dataSet[0]) - 1  # 最后一列是label
        baseEntropy = calcShannonEnt(dataSet)  # 计算当前数据集的熵
        bestInfoGain = 0.0; bestFeature = -1  # bestInfoGain为当前熵值差 bestFeature为最好的标签
        for i in range(lenOfFeatures): # 需要计算每一列对应的熵
            featList = [example[i] for example in dataSet]
            uniqueVals = set(featList)
            newEntropy = 0.0
            # 求每一列的总熵值
            for value in uniqueVals:
                splitData = splitDataSet(dataSet, i, value)
                prob = len(splitData) / len(dataSet)
                newEntropy += (prob * calcShannonEnt(splitData))
            if baseEntropy - newEntropy > bestInfoGain:  # 信息增益是熵的减少或者是数据无序度的减少
                bestInfoGain = baseEntropy - newEntropy
                bestFeature = i
        return bestFeature


    # 多数表决的方法决定该叶子结点的分类
    def majorityCnt(classList):
        classCount = {}
        for label in classList:
            classCount[label] = classCount.get(label, 0) + 1
        sortedClassCount = sorted(classCount.items(), key=lambda x: x[1], reverse=True)
        return sortedClassCount[0][0]


    # 创建树
    def createTree(dataSet, labels):
        classList = [example[-1] for example in dataSet]
        if len(set(classList)) == 1:
            return classList[0]
        if len(dataSet[0]) == 1:
            return majorityCnt(classList)
        bestFeat = chooseBestFeatureToSplit(dataSet)
        bestFeatLabel = labels[bestFeat]
        myTree = {bestFeatLabel: {}}
        del labels[bestFeat]
        featValues = [example[bestFeat] for example in dataSet]
        uniqueVals = set(featValues)
        for value in uniqueVals:
            subLabels = labels[:]
            myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)
        return myTree


    def createDataSet():
        dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
        labels = ['no surfacing', 'flippers']
        return dataSet, labels


    myDat, labels = createDataSet()
    print(createTree(myDat, labels))
